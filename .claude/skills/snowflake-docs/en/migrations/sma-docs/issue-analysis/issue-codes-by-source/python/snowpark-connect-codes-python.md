---
auto_generated: true
description: Message The element < element > is not supported for Snowpark Connect
last_scraped: '2026-01-14T16:51:27.554124+00:00'
scraper_version: 1.1.0
source_url: https://docs.snowflake.com/en/migrations/sma-docs/issue-analysis/issue-codes-by-source/python/snowpark-connect-codes-python
title: 'Snowpark Migration Accelerator: Snowpark Connect Issue Codes for Python |
  Snowflake Documentation'
---

1. [Overview](../../../../../guides/README.md)
2. [Snowflake Horizon Catalog](../../../../../user-guide/snowflake-horizon.md)
4. [Applications and tools for connecting to Snowflake](../../../../../guides/overview-connecting.md)
6. [Virtual warehouses](../../../../../user-guide/warehouses.md)
7. [Databases, Tables, & Views](../../../../../guides/overview-db.md)
8. [Data types](../../../../../data-types.md)
10. Data Integration

    - [Snowflake Openflow](../../../../../user-guide/data-integration/openflow/about.md)
    - Apache Iceberg™

      - [Apache Iceberg™ Tables](../../../../../user-guide/tables-iceberg.md)
      - [Snowflake Open Catalog](../../../../../user-guide/opencatalog/overview.md)
11. Data engineering

    - [Data loading](../../../../../guides/overview-loading-data.md)
    - [Dynamic Tables](../../../../../user-guide/dynamic-tables-about.md)
    - [Streams and Tasks](../../../../../user-guide/data-pipelines-intro.md)
    - [dbt Projects on Snowflake](../../../../../user-guide/data-engineering/dbt-projects-on-snowflake.md)
    - [Data Unloading](../../../../../guides/overview-unloading-data.md)
12. [Storage Lifecycle Policies](../../../../../user-guide/storage-management/storage-lifecycle-policies.md)
13. [Migrations](../../../../README.md)

    * Tools

      * [SnowConvert AI](../../../../snowconvert-docs/overview.md)
      * [Snowpark Migration Accelerator](../../../README.md)

        + General

          + [Introduction](../../../general/introduction.md)
          + [Getting started](../../../general/getting-started/README.md)
          + [Conversion software terms of use](../../../general/conversion-software-terms-of-use/README.md)
          + [Release notes](../../../general/release-notes/README.md)
          + [Roadmap](../../../general/roadmap.md)
        + User guide

          + [Overview](../../../user-guide/overview.md)
          + [Before using the SMA](../../../user-guide/before-using-the-sma/README.md)
          + [Project overview](../../../user-guide/project-overview/README.md)
          + [Technical discovery](../../../user-guide/project-overview/optional-technical-discovery.md)
          + [AI assistant](../../../user-guide/chatbot.md)
          + [Assessment](../../../user-guide/assessment/README.md)
          + [Conversion](../../../user-guide/conversion/README.md)
          + [Using the SMA CLI](../../../user-guide/using-the-sma-cli/README.md)
        + Use cases

          + Snowflake VS Code extension

            + [SMA checkpoints walkthrough](../../../use-cases/sma-checkpoints-walkthrough/README.md)
            + [SMA EWI Assistant walkthrough](../../../use-cases/sma-ewi-assistant-walkthrough/README.md)
          + [Assessment walkthrough](../../../use-cases/assessment-walkthrough/README.md)
          + [Conversion walkthrough](../../../use-cases/conversion-walkthrough.md)
          + [Migration lab](../../../use-cases/migration-lab/README.md)
          + [Sample project](../../../use-cases/sample-project.md)
          + [Using SMA in an Ubuntu Docker image](../../../use-cases/using-snowconvert-in-a-ubuntu-docker-image.md)
          + [SMA CLI walkthrough](../../../use-cases/sma-cli-walkthrough.md)
          + [Snowpark Connect](../../../use-cases/snowpark-connect/README.md)
        + Issue analysis

          + [Approach](../../approach.md)
          + [Issue code categorization](../../issue-code-categorization.md)
          + [Issue codes by source](../README.md)

            - [General](../general.md)
            - [Python](README.md)
            - [Spark Scala](../spark-scala/README.md)
            - [SQL](../sql/README.md)
            - [Pandas](../pandas/README.md)
            - [Snowpark Connect Python](snowpark-connect-codes-python.md)
            - [Snowpark Connect Scala](../spark-scala/snowpark-connect-codes-scala.md)
          + [Troubleshooting the output code](../../troubleshooting-the-output-code/README.md)
          + [Workarounds](../../workarounds.md)
          + [Deploying the output code](../../deploying-the-output-code.md)
        + Translation reference

          + [Translation reference overview](../../../translation-reference/translation-reference-overview.md)
          + [SIT tagging](../../../translation-reference/sit-tagging/README.md)
          + [SQL embedded code](../../../translation-reference/sql-embedded-code.md)
          + [HiveSQL](../../../translation-reference/hivesql/README.md)
          + [Spark SQL](../../../translation-reference/spark-sql/README.md)
        + Workspace estimator

          + [Overview](../../../workspace-estimator/overview.md)
          + [Getting started](../../../workspace-estimator/getting-started.md)
        + Interactive assessment application

          + [Overview](../../../interactive-assessment-application/overview.md)
          + [Installation guide](../../../interactive-assessment-application/installation-guide.md)
        + Support

          + [General troubleshooting](../../../support/general-troubleshooting/README.md)
          + [Frequently asked questions](../../../support/frequently-asked-questions-faq/README.md)
          + [Glossary](../../../support/glossary.md)
          + [Contact us](../../../support/contact-us.md)
    * Guides

      * [Teradata](../../../../guides/teradata.md)
      * [Databricks](../../../../guides/databricks.md)
      * [SQL Server](../../../../guides/sqlserver.md)
      * [Amazon Redshift](../../../../guides/redshift.md)
      * [Oracle](../../../../guides/oracle.md)
      * [Azure Synapse](../../../../guides/azuresynapse.md)
15. [Queries](../../../../../guides/overview-queries.md)
16. [Listings](../../../../../collaboration/collaboration-listings-about.md)
17. [Collaboration](../../../../../guides/overview-sharing.md)
19. [Snowflake AI & ML](../../../../../guides/overview-ai-features.md)
21. [Snowflake Postgres](../../../../../user-guide/snowflake-postgres/about.md)
23. [Alerts & Notifications](../../../../../guides/overview-alerts.md)
25. [Security](../../../../../guides/overview-secure.md)
26. [Data Governance](../../../../../guides/overview-govern.md)
27. [Privacy](../../../../../guides/overview-privacy.md)
29. [Organizations & Accounts](../../../../../guides/overview-manage.md)
30. [Business continuity & data recovery](../../../../../user-guide/replication-intro.md)
32. [Performance optimization](../../../../../guides/overview-performance.md)
33. [Cost & Billing](../../../../../guides/overview-cost.md)

[Guides](../../../../../guides/README.md)[Migrations](../../../../README.md)Tools[Snowpark Migration Accelerator](../../../README.md)Issue analysis[Issue codes by source](../README.md)Snowpark Connect Python

# Snowpark Migration Accelerator: Snowpark Connect Issue Codes for Python[¶](#snowpark-migration-accelerator-snowpark-connect-issue-codes-for-python "Link to this heading")

## SPRKCNTPY1000[¶](#sprkcntpy1000 "Link to this heading")

**Message** The element < ***element*** > is not supported for Snowpark Connect

**Category** Conversion Error

### Description[¶](#description "Link to this heading")

This issue appears when the tool detects the usage of an element that is not supported in Snowpark Connect, and does not have its own error code associated with it. This is the generic error code used by the SMA for an unsupported element.

### Scenario[¶](#scenario "Link to this heading")

#### Input[¶](#input "Link to this heading")

The tool found an unidentified element of the pyspark library.

```
from pyspark import NotSupportedElement
sc.addFile("data.txt")
print(NotSupportedElement.get("data.txt"))
```

Copy

#### Output[¶](#output "Link to this heading")

The tool adds the comment to the statement pointing to the unsupported element.

```
from pyspark import NotSupportedElement
sc.addFile("data.txt")
# EWI SPRKCNTPY1000: The element 'NotSupportedElement' is not supported for Snowpark Connect
print(NotSupportedElement.get("data.txt"))
```

Copy

### Recommended fix[¶](#recommended-fix "Link to this heading")

Since this is a generic error code that applies to a range of unsupported functions, there is not a single and specific fix. The appropriate action will depend on the particular element in use.

### Additional recommendations[¶](#additional-recommendations "Link to this heading")

* Even though the option or the element on the message is not supported, this does not mean that a solution cannot be found. It means only that the tool itself cannot find the solution.
* If you believe that Snowpark Connect already supports this element or that there is some kind of workaround, please report that you encountered a conversion error on that particular element using the [Report an Issue option](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue) in the SMA and include any additional information that you think may be helpful.
* For more support, email support at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com) or post an issue [in the SMA](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue).

## SPRKCNTPY1001[¶](#sprkcntpy1001 "Link to this heading")

`SparkSession` has been replaced with `Session` in Snowpark Connect.

**Message** The `SparkSession` creation has been transformed to use a Snowpark `Session` equivalent.

**Category** Warning.

### Description[¶](#id1 "Link to this heading")

This issue appears when the SMA detects the creation of a `SparkSession` object in the input code. Snowpark Connect uses a different object, called `Session`, to manage the connection to Snowflake and to create DataFrames.

When the SMA encounters the creation of a `SparkSession`, it adds this EWI to inform you that it has transformed the code to use a Snowpark Session instead.

### Scenario[¶](#id2 "Link to this heading")

#### Input[¶](#id3 "Link to this heading")

Below is an example of a Python `SparkSession` initialization which will be replaced for a Snowpark Connect `Session` initialization, and therefore it generates this EWI.

```
spark = SparkSession.builder.getOrCreate()
```

Copy

#### Output[¶](#id4 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY1001` to the output code to let you know that your `SparkSession` initialization has been replaced for a Snowpark Connect `Session` initialization.

```
#EWI: SPRKCNTPY1001 => The creation of the SparkSession has been replaced with the creation of an equivalent Snowpark Connect Session.
spark = snowpark_connect.server.init_spark_session()
```

Copy

### Additional recommendations[¶](#id5 "Link to this heading")

* For more support, you can email us at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com). If you have a contract for support with Snowflake, reach out to your sales engineer, and they can direct your support needs.

## SPRKCNTPY1500[¶](#sprkcntpy1500 "Link to this heading")

**Message** The element < ***element*** > of the library RDD is not supported for Snowpark Connect.

**Category** Conversion Error

### Description[¶](#id6 "Link to this heading")

This issue appears when the tool determines that the usage instance of an RDD element is not supported in Snowpark Connect, and does not have its own error code associated with it. This is the generic error code used by the SMA for RDD unsupported elements.

### Scenario[¶](#id7 "Link to this heading")

#### Input[¶](#id8 "Link to this heading")

The tool found an unidentified element of the `pyspark.rdd` library.

```
from pyspark import SparkContext

sc = SparkContext("local", "Simple App")
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
result = rdd.NotSupportedElement(withReplacement=False, fraction=0.5).collect()
print(result)
```

Copy

#### Output[¶](#id9 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY1500` to the output code to let you know that this RDD element is not supported by Snowpark Connect.

```
from pyspark import SparkContext

sc = SparkContext("local", "Simple App")
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
# EWI SPRKCNTPY1500: The element 'NotSupportedElement' of the library RDD is not supported for Snowpark Connect
result = rdd.NotSupportedElement(withReplacement=False, fraction=0.5).collect()
print(result)
```

Copy

### Recommended fix[¶](#id10 "Link to this heading")

* Convert RDD operations to DataFrame operations.

  ```
  # PySpark RDD approach (NOT supported)
  rdd = spark.sparkContext.parallelize(data)
  result = rdd.map(lambda x: x * 2).collect()

  # Snowpark DataFrame approach (Supported)
  df = session.create_dataframe(data, schema=["value"])
  result = df.select(col("value") * 2).collect()
  ```

  Copy
* Process data locally before sending it to Snowflake if the RDD operations are simple.
* Use pandas DataFrames for local processing, then convert to Snowpark DataFrames.
* Since this is a generic error code that applies to a range of unsupported functions, there is no single and specific fix. The appropriate action will depend on the particular element in use.
* Please note that even though the element is not supported, it does not necessarily mean that a solution or workaround cannot be found. It means only that the SMA itself cannot find the solution.

### Additional recommendations[¶](#id11 "Link to this heading")

* Even though the option or the element on the message is not supported, this does not mean that a solution cannot be found. It means only that the tool itself cannot find the solution.
* If you believe that Snowpark Connect already supports this element or that there is a workaround, please report that you encountered a conversion error on that particular element using [the Report an Issue option](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue) in the SMA and include any additional information that you think may be helpful.
* For more support, email support at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com) or post an issue [in the SMA](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue).

## SPRKCNTPY2000[¶](#sprkcntpy2000 "Link to this heading")

**Message** The element < ***element*** > of the library Streaming is not supported for Snowpark Connect.

**Category** Conversion Error

### Description[¶](#id12 "Link to this heading")

This issue appears when the tool determines that the usage instance of a Streaming element is not supported in Snowpark Connect and does not have its own error code associated with it. This is the generic error code used by the SMA for Streaming unsupported elements.

### Scenario[¶](#id13 "Link to this heading")

#### Input[¶](#id14 "Link to this heading")

The tool found an unidentified element of the `pyspark.streaming` library.

```
from pyspark import SparkContext
from pyspark.streaming import NotSupportedElement

sc = SparkContext("local[2]", "NetworkWordCount")
ssc = NotSupportedElement(sc, 1)
```

Copy

#### Output[¶](#id15 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY2000` to the output code to let you know that this Streaming element is not supported by Snowpark Connect.

```
from pyspark import SparkContext
from pyspark.streaming import NotSupportedElement

sc = SparkContext("local[2]", "NetworkWordCount")
# EWI SPRKCNTPY2000: The element 'NotSupportedElement' of the library Streaming is not supported for Snowpark Connect
ssc = NotSupportedElement(sc, 1)
```

Copy

### Recommended fix[¶](#id16 "Link to this heading")

* Use Snowflake Streams to capture table changes.
* Process changes with Snowpark Connect in scheduled jobs.
* Ideal for maintaining derived tables.
* Since this is a generic error code that applies to a range of unsupported functions, there no single and specific fix. The appropriate action will depend on the particular element in use.
* Please note that even though the element is not supported, it does not necessarily mean that a solution or workaround cannot be found. It means only that the SMA itself cannot find the solution.

### Additional recommendations[¶](#id17 "Link to this heading")

* Even though the option or the element on the message is not supported, this does not mean that a solution cannot be found. It means only that the tool itself cannot find the solution.
* If you believe that Snowpark Connect already supports this element or that there is a workaround, please report that you encountered a conversion error on that particular element using [the Report an Issue option](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue) in the SMA and include any additional information that you think may be helpful.
* For more support, email support at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com) or post an issue [in the SMA](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue).

## SPRKCNTPY2500[¶](#sprkcntpy2500 "Link to this heading")

**Message** The element < ***element*** > of the library ML is not supported for Snowpark Connect.

**Category** Conversion Error

### Description[¶](#id18 "Link to this heading")

This issue appears when the tool determines the usage instance of an ML element that is not supported in Snowpark Connect and does not have its own error code associated with it. This is the generic error code used by the SMA for ML unsupported elements.

### Scenario[¶](#id19 "Link to this heading")

#### Input[¶](#id20 "Link to this heading")

The tool found an unidentified element of the `pyspark.ml` library.

```
from pyspark.ml.classification import NotSupportedElement

lr = NotSupportedElement(maxIter=10, regParam=0.01)
model = lr.fit(trainingData)
```

Copy

#### Output[¶](#id21 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY2500` to the output code to let you know that this ML element is not supported by Snowpark Connect.

```
from pyspark.ml.classification import NotSupportedElement

# EWI SPRKCNTPY2500: The element 'NotSupportedElement' of the library ML is not supported for Snowpark Connect
lr = NotSupportedElement(maxIter=10, regParam=0.01)
model = lr.fit(trainingData)
```

Copy

### Recommended fix[¶](#id22 "Link to this heading")

* Use the Snowpark ML library.

  ```
  # Instead of PySpark ML
  from snowflake.ml.modeling.linear_model import LinearRegression
  from snowflake.ml.modeling.ensemble import RandomForestRegressor

  # Snowpark ML approach
  model = LinearRegression()
  model.fit(train_df)
  predictions = model.predict(test_df)
  ```

  Copy
* Since this is a generic error code that applies to a range of unsupported functions, there is no single and specific fix. The appropriate action will depend on the particular element in use.
* Please note that even though the element is not supported, it does not necessarily mean that a solution or workaround cannot be found. It means only that the SMA itself cannot find the solution.

### Additional recommendations[¶](#id23 "Link to this heading")

* Even though the option or the element on the message is not supported, this does not mean that a solution cannot be found. It means only that the tool itself cannot find the solution.
* If you believe that Snowpark Connect already supports this element or that there is some kind of workaround, please report that you encountered a conversion error on that particular element using [the Report an Issue option](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue) in the SMA and include any additional information that you think may be helpful.
* For more support, email support at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com) or post an issue [in the SMA](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue).

## SPRKCNTPY3000[¶](#sprkcntpy3000 "Link to this heading")

**Message** The element < ***element*** > of the library MLLIB is not supported for Snowpark Connect

**Category** Conversion Error

### Description[¶](#id24 "Link to this heading")

This issue appears when the tool determines the usage instance of an MLLIB element that is not supported in Snowpark Connect, and does not have its own error code associated with it. This is the generic error code used by the SMA for ML unsupported elements.

### Scenario[¶](#id25 "Link to this heading")

#### Input[¶](#id26 "Link to this heading")

The tool found an unidentified element of the `pyspark.mllib` library.

```
from pyspark.mllib.recommendation import NotSupportedElement, Rating
ratings = [
    Rating(0, 0, 4.0),
    Rating(0, 1, 2.0),
    Rating(1, 1, 5.0)
]
model = NotSupportedElement.train(ratings, 10)
```

Copy

#### Output[¶](#id27 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY3000` to the output code to let you know that this MLLIB element is not supported by Snowpark Connect.

```
from pyspark.mllib.recommendation import NotSupportedElement, Rating

# EWI SPRKCNTPY3000: The element 'NotSupportedElement' of the library MLLIB is not supported for Snowpark Connect
ratings = [
    Rating(0, 0, 4.0),
    Rating(0, 1, 2.0),
    Rating(1, 1, 5.0)
]
model = NotSupportedElement.train(ratings, 10)
```

Copy

### Recommended fix[¶](#id28 "Link to this heading")

* Use the Snowpark ML library.

  ```
  # Instead of MLlib's LinearRegressionWithSGD
  from snowflake.ml.modeling.linear_model import LinearRegression

  # MLlib approach (NOT supported)
  # from pyspark.mllib.regression import LinearRegressionWithSGD
  # model = LinearRegressionWithSGD.train(rdd_data)

  # Snowpark ML approach
  model = LinearRegression(input_cols=['feature1', 'feature2'], label_cols=['target'])
  model.fit(train_df)
  ```

  Copy
* Since this is a generic error code that applies to a range of unsupported functions, there is no single and specific fix. The appropriate action will depend on the particular element in use.
* Please note that even though the element is not supported, it does not necessarily mean that a solution or workaround cannot be found. It means only that the SMA itself cannot find the solution.

### Additional recommendations[¶](#id29 "Link to this heading")

* Even though the option or the element on the message is not supported, this does not mean that a solution cannot be found. It means only that the tool itself cannot find the solution.
* If you believe that Snowpark Connect already supports this element or that there is a workaround, please report that you encountered a conversion error on that particular element using [the Report an Issue option](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue) in the SMA and include any additional information that you think may be helpful.
* For more support, email support at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com) or post an issue [in the SMA](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue).

## SPRKCNTPY3500[¶](#sprkcntpy3500 "Link to this heading")

**Message** The element < ***element*** > of the library Spark Session is not supported for Snowpark Connect.

**Category** Conversion Error

### Description[¶](#id30 "Link to this heading")

This issue appears when the tool determines that the usage instance of a Spark Session element is not supported in Snowpark Connect and does not have its own error code associated with it. This is the generic error code used by the SMA for ML unsupported elements.

### Scenario[¶](#id31 "Link to this heading")

#### Input[¶](#id32 "Link to this heading")

The tool found an unidentified element of the `pyspark.SparkSession` library.

```
from pyspark.sql import SparkSession
from pyspark.sql.SparkSession import NotSupportedElement

spark = SparkSession.builder.appName("Example").getOrCreate()
new_spark = spark.NotSupportedElement()
```

Copy

#### Output[¶](#id33 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY3500` to the output code to let you know that this Spark Session element is not supported by Snowpark Connect.

```
from pyspark.sql import SparkSession
from pyspark.sql.SparkSession import NotSupportedElement

spark = SparkSession.builder.appName("Example").getOrCreate()
# EWI SPRKCNTPY3500: The element 'NotSupportedElement' of the library Spark Session is not supported for Snowpark Connect
new_spark = spark.NotSupportedElement()
```

Copy

### Recommended fix[¶](#id34 "Link to this heading")

* The key is to replace `SparkSession` patterns with equivalent Snowpark `Session` operations while leveraging Snowflake’s unique capabilities like warehouses, stages, and native SQL functions.

  ```
  # PySpark SparkSession
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.appName("MyApp").getOrCreate()

  # Snowpark Session
  from snowflake.snowpark import Session
  session = Session.builder.configs({
      "account": "your_account",
      "user": "your_user",
      "password": "your_password",
      "role": "your_role",
      "warehouse": "your_warehouse",
      "database": "your_database",
      "schema": "your_schema"
  }).create()
  ```

  Copy
* Since this is a generic error code that applies to a range of unsupported functions, there is no single and specific fix. The appropriate action will depend on the particular element in use.
* Please note that even though the element is not supported, it does not necessarily mean that a solution or workaround cannot be found. It means only that the SMA itself cannot find the solution.

### Additional recommendations[¶](#id35 "Link to this heading")

* Even though the option or the element on the message is not supported, this does not mean that a solution cannot be found. It means only that the tool itself cannot find the solution.
* If you believe that Snowpark Connect already supports this element or that there is some kind of workaround, please report that you encountered a conversion error on that particular element using [the Report an Issue option](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue) in the SMA and include any additional information that you think may be helpful.
* For more support, email support at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com) or post an issue [in the SMA](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue).

## SPRKCNTPY3501[¶](#sprkcntpy3501 "Link to this heading")

The `AppName` method of `pyspark.sql.session.SparkSession.Builder` has been replaced with the `SetName` function to provide equivalent functionality in Snowpark Connect when creating a session.

**Message** The `AppName` method of `pyspark.sql.session.SparkSession.Builder` has been replaced with the `SetName` function to provide equivalent functionality in Snowpark Connect.

**Category** Warning.

### Description[¶](#id36 "Link to this heading")

This issue occurs when the SMA detects the use of `AppName` Spark function while creating a `SparkSession` instance. The tool replaces this function with Snowpark initialization statements to achieve equivalent functionality.

### Scenario[¶](#id37 "Link to this heading")

#### Input[¶](#id38 "Link to this heading")

Below is an example of a Python `AppName` Spark function that will be replaced by the `SetAppName` Snowpark Connect function, and therefore it will add this EWI.

```
spark = (
  SparkSession
    .builder
    .appName("MyApp")
    .getOrCreate()
)
```

Copy

#### Output[¶](#id39 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY3501` to the output code to let you know that this element has been transformed.

```
conf = SparkConf()
conf.setAppName("MyApp")
#EWI: SPRKCNTPY3501 => The AppName method of pyspark.sql.session.SparkSession.Builder has been replaced with the SetAppName function to provide equivalent functionality in Snowpark Connect

spark = ( snowpark_connect.server.init_spark_session(conf = conf)
)
```

Copy

### Recommended fix[¶](#id40 "Link to this heading")

Review the output code and ensure that the Snowpark Connect session is configured correctly with the desired application name.

### Additional recommendations[¶](#id41 "Link to this heading")

* Please review the Snowpark Connect documentation to understand how to configure and use Snowpark Connect sessions effectively.
* For more support, you can email us at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com). If you have a contract for support with Snowflake, reach out to your sales engineer, and they can direct your support needs.

## SPRKCNTPY3502[¶](#sprkcntpy3502 "Link to this heading")

The `Master` method of `pyspark.sql.session.SparkSession.Builder` has been replaced with the `SetName` function to provide equivalent functionality in Snowpark Connect when creating a session.

**Message** The `Master` method of `pyspark.sql.session.SparkSession.Builder` has been replaced with the `SetName` function to provide equivalent functionality in Snowpark Connect.

**Category** Warning.

### Description[¶](#id42 "Link to this heading")

This issue occurs when the SMA detects the use of `Master` Spark function while creating a `SparkSession` instance. The tool replaces this function with Snowpark initialization statements to achieve equivalent functionality.

### Scenario[¶](#id43 "Link to this heading")

#### Input[¶](#id44 "Link to this heading")

Below is an example of a Python `Master` Spark function that will be replaced by the `SetAppName` Snowpark Connect function, and therefore it will add this EWI.

```
spark = (
  SparkSession
    .builder
    .master("local[1]")
    .getOrCreate()
)
```

Copy

#### Output[¶](#id45 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY3502` to the output code to let you know that this element has been transformed.

```
conf = SparkConf()
conf.setMaster("local[1]")
#EWI: SPRKCNTPY3502 => The Master method of pyspark.sql.session.SparkSession.Builder has been replaced with the SetMaster function to provide equivalent functionality in Snowpark Connect

spark = ( snowpark_connect.server.init_spark_session(conf = conf)
)
```

Copy

### Recommended fix[¶](#id46 "Link to this heading")

Review the output code and ensure that the Snowpark Connect session is configured correctly with the desired master setting.

### Additional recommendations[¶](#id47 "Link to this heading")

* Please review the Snowpark Connect documentation to understand how to configure and use Snowpark Connect sessions effectively.
* For more support, you can email us at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com). If you have a contract for support with Snowflake, reach out to your sales engineer, and they can direct your support needs.

## SPRKCNTPY4000[¶](#sprkcntpy4000 "Link to this heading")

SparkContext element is not supported by Snowpark Connect.

**Message** The element ***<element full name>*** of the library `SparkContext` is not supported by Snowpark Connect.

**Category** Conversion error

### Description[¶](#id48 "Link to this heading")

This issue appears when the SMA detects an usage of a Python `SparkContext` element that is not supported by Snowpark Connect and does not have its own specific error code associated with it. This is a generic error code used by the SMA for unsupported `SparkContext` elements.

### Scenario[¶](#id49 "Link to this heading")

#### Input[¶](#id50 "Link to this heading")

Below is an example of an usage of a `SparkContext` element that triggers this EWI:

```
from pyspark import SparkContext

sc = SparkContext()
sc.not_supported_element()
```

Copy

#### Output[¶](#id51 "Link to this heading")

The SMA adds the EWI `SPRKCNTPY4000` indicating that the `SparkContext` element is not supported by Snowpark Connect.

```
from pyspark import SparkContext

sc = SparkContext()
#EWI: SPRKCNTPY4000 => The element 'pyspark.context.SparkContext.not_supported_element' of the library SparkContext is not supported by Snowpark Connect
sc.not_supported_element()
```

Copy

### Recommended fix[¶](#id52 "Link to this heading")

Snowpark Connect uses a DataFrame-based architecture and doesn’t support `SparkContext` or RDD operations. As a workaround, you could refactor your code to use Snowpark Connect `Session` and `DataFrame` APIs instead.

### Additional recommendations[¶](#id53 "Link to this heading")

* Consult the [Snowpark Connect documentation](../../../../../developer-guide/snowpark-connect/snowpark-connect-overview) for available alternatives to your specific use case.
* Note that some SparkContext functionality has no direct equivalent and may require application redesign.
* For more support, you can email us at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com) or post an issue [in the SMA](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue).

## SPRKCNTPY4001[¶](#sprkcntpy4001 "Link to this heading")

**Message** `SparkContext` instantiation has been converted to a Snowpark Connect session.

**Category** Warning

### Description[¶](#id54 "Link to this heading")

This issue appears when the SMA detects [pyspark.context.SparkContext](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html) constructor calls in your code. The SMA automatically transforms these instantiations into equivalent Snowpark Connect session calls, enabling your Spark applications to run on Snowflake’s infrastructure.

The transformation process involves the following steps:

* Replacing `SparkContext` instantiation with `snowpark_connect.server.init_spark_session()`
* Preserving any existing `SparkConf` configuration parameters

> **Important**: Before running the converted code, you **must** configure your connection details in a `connections.toml` or `config.toml` file. This configuration file should contain your Snowflake account credentials, warehouse information, and other connection parameters required for Snowpark Connect to establish a connection to your Snowflake account.
>
> For comprehensive setup instructions, please refer to the [official Snowpark Connect documentation](../../../../../developer-guide/snowpark-connect/snowpark-connect-workloads-jupyter).

### Scenarios[¶](#scenarios "Link to this heading")

#### Scenario 1[¶](#scenario-1 "Link to this heading")

##### Input code[¶](#input-code "Link to this heading")

`SparkContext` instantiated with default parameters:

```
sc = SparkContext()
```

Copy

##### Output code[¶](#output-code "Link to this heading")

The SMA sets the environment variable, starts the Snowpark Connect session, and retrieves the session without additional configuration:

```
#EWI: SPRKCNTPY4001 => SparkContext instantiation has been converted to a Snowpark Connect session
sc = snowpark_connect.server.init_spark_session()
```

Copy

#### Scenario 2[¶](#scenario-2 "Link to this heading")

##### Input code[¶](#id55 "Link to this heading")

SparkContext instantiated with master and `appName` parameters:

```
sc = SparkContext(master="local[*]", appName="MyApp")
# or
sc = SparkContext("local[*]", "MyApp")
```

Copy

##### Output code[¶](#id56 "Link to this heading")

The SMA sets the environment variable, starts the Snowpark Connect session, and passes the parameters via a `SparkConf` object:

```
conf = SparkConf()
conf.setAppName("MyApp")
conf.setMaster("local[*]")
#EWI: SPRKCNTPY4001 => SparkContext instantiation has been converted to a Snowpark Connect session
sc = snowpark_connect.server.init_spark_session(conf = conf)
```

Copy

#### Scenario 3[¶](#scenario-3 "Link to this heading")

##### Input code[¶](#id57 "Link to this heading")

`SparkContext` instantiated using an existing `SparkConf` object.

```
my_conf = SparkConf()
sc = SparkContext(conf=my_conf)
```

Copy

##### Output code[¶](#id58 "Link to this heading")

The SMA preserves the existing `SparkConf` object and passes it directly to the `snowpark_connect.server.init_spark_session()` method:

```
my_conf = SparkConf()
#EWI: SPRKCNTPY4001 => SparkContext instantiation has been converted to a Snowpark Connect session
sc = snowpark_connect.server.init_spark_session(conf = my_conf)
```

Copy

### Additional Recommendations[¶](#id59 "Link to this heading")

* While the SMA preserves your `SparkConf` settings, not all Spark configurations may be supported in Snowpark Connect. Review your configurations to ensure compatibility.
* Ensure that downstream operations using the `SparkContext` object are compatible with Snowpark Connect, as some Spark-specific functionalities may not have direct equivalents.
* For more support, you can email us at [sma-support@snowflake.com](mailto:sma-support%40snowflake.com) or post an issue [in the SMA](../../../user-guide/project-overview/configuration-and-settings.html#report-an-issue).

Was this page helpful?

YesNo

[Visit Snowflake](https://www.snowflake.com)

[Join the conversation](https://community.snowflake.com/s/)

[Develop with Snowflake](https://developers.snowflake.com)

[Share your feedback](/feedback)

[Read the latest on our blog](https://www.snowflake.com/blog/)

[Get your own certification](https://learn.snowflake.com)

[Privacy Notice](https://www.snowflake.com/privacy-policy/)[Site Terms](https://www.snowflake.com/legal/snowflake-site-terms/)Cookies Settings© 2026 Snowflake, Inc. All Rights Reserved.

On this page

1. [SPRKCNTPY1000](#sprkcntpy1000)
2. [SPRKCNTPY1001](#sprkcntpy1001)
3. [SPRKCNTPY1500](#sprkcntpy1500)
4. [SPRKCNTPY2000](#sprkcntpy2000)
5. [SPRKCNTPY2500](#sprkcntpy2500)
6. [SPRKCNTPY3000](#sprkcntpy3000)
7. [SPRKCNTPY3500](#sprkcntpy3500)
8. [SPRKCNTPY3501](#sprkcntpy3501)
9. [SPRKCNTPY3502](#sprkcntpy3502)
10. [SPRKCNTPY4000](#sprkcntpy4000)
11. [SPRKCNTPY4001](#sprkcntpy4001)