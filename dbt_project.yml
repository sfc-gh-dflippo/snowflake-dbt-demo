# Name your project! Project names should contain only lowercase characters
# and underscores. A good package name should reflect your organization's
# name or the intended use of these models
name: "snowflake_demo"
version: "1.0.0"
config-version: 2

# v1.0.0: The config source-paths has been deprecated in favor of model-paths
# v1.0.0: The clean-target dbt_modules has been deprecated in favor of dbt_packages
# v1.6.0: We are now utilizing the new support for dynamic_tables
# v1.7.0: There were several important bugs fixed for dynamic tables
require-dbt-version: ">=1.7.0"

# This setting configures which "profile" dbt uses for this project.
profile: "SNOWFLAKE"

# The query-comment setting is used to add a comment to the SQL query.
# This is useful for debugging and for tracking the query history.
# Setting it to null will not add a comment to the SQL query.
query-comment: null

# These configurations specify where dbt should look for different types of files.
# The `source-paths` config, for example, states that models in this project can be
# found in the "models/" directory. You probably won't need to change these!
#model-paths: ["models"]
#analysis-paths: ["analysis"]
#test-paths: ["tests"]
#seed-paths: ["data"]
#macro-paths: ["macros"]
#snapshot-paths: ["snapshots"]
#target-path: "target"  # directory which will store compiled SQL files
clean-targets: # directories to be removed by `dbt clean`
  - "target"
  - "dbt_packages" # Comment to not clean up dependencies
  - "dbt_internal_packages"
  - "logs"

# Global initialization and shutdown commands
# These are not required but will allow us to reduce consumption to the second
on-run-start:
  - "{{ create_masking_policies() }}"

on-run-end:
  # - "{{ dbt_artifacts.upload_results(results) }}"
  #- "ALTER WAREHOUSE DFLIPPO_DBT_XLARGE_DEMO_WH SUSPEND"
  - "{% if target.name == 'prod' %}{{ dbt_artifacts.upload_results(results) }}{% endif %}"

# Global variables
vars:
  # The number of days that we want to reprocess data when doing incremental loads
  prune_days: 2
  default_integration_key: integration_id
  default_scd_cdc_hash_key: cdc_hash_key
  default_updated_at_column: dbt_updated_ts
  default_inserted_at_column: dbt_inserted_ts
  
  # dbt-project-evaluator configuration
  dbt_project_evaluator:
    # Configure primary key test recognition for dbt_constraints package
    primary_key_test_macros: [
      ["dbt_constraints.test_primary_key"],
      ["dbt_constraints.test_unique_key", "dbt.test_not_null"],
      ["dbt.test_unique", "dbt.test_not_null"]
    ]

# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt by default to build all models as views.
# These settings can be overridden at lower levels and in the individual model files
# using the `{{ config(...) }}` macro.

models:
  # You can set custom properties on models that will be stored in the dbt logs
  +meta:
    owner: "Dan Flippo"
    owner_email: "test@nowhere.com"

  #pre-hook:
  # - "{{ set_warehouse() }}"

  # Config indicated by + applies to all files at and below that level
  #+post-hook:
    # - "{{ dbt_snow_mask.apply_masking_policy() }}"
    # This macro will refresh dynamic tables as they are processed
    # - "{{ dynamic_table_refresh_hook() }}"

  # By default grants are lost unless you add this configuration
  +copy_grants: true
  # +snowflake_warehouse: "DFLIPPO_XSMALL_WH"
  # Setting the warehouse here will avoid errors for dynamic table models that don't specify a warehouse
  +snowflake_warehouse: "{{ target.warehouse }}"
  +materialized: view
  
  snowflake_demo:
    # We will store table and column descriptions in Snowflake
    +persist_docs:
      relation: true
      columns: true
    
    # MEDALLION ARCHITECTURE WITH INTEGRATED BEST PRACTICES
    # Bronze Layer - Staging models + raw data patterns
    bronze:
      +tags: ["bronze", "staging", "raw_data"]
      +schema: bronze
      +materialized: ephemeral  # Best practice: staging as ephemeral for performance
      crawl:
        +tags: ["crawl", "beginner"]
      walk:
        +tags: ["walk", "intermediate"] 
      run:
        +tags: ["run", "advanced"]
    
    # Silver Layer - Intermediate models + transformation patterns
    silver:
      +tags: ["silver", "intermediate", "transformed_data"]
      +schema: silver
      crawl:
        +tags: ["crawl", "beginner"]
        +materialized: table
      walk:
        +tags: ["walk"]
        +materialized: ephemeral  # Best practice: intermediate as ephemeral
      run:
        +tags: ["run", "advanced"]
        +materialized: table  # Some complex intermediate models need persistence
        +snowflake_warehouse: "{{ target.warehouse }}"
    
    # Gold Layer - Marts models + business analytics
    gold:
      +tags: ["gold", "marts", "business_ready"]
      +schema: gold
      +materialized: table  # Best practice: marts as tables
      crawl:
        +tags: ["crawl", "beginner"]
      walk:
        +tags: ["walk", "intermediate"]
        sensitive_data:
          +tags: ["sensitive"]
          +materialized: view
          +secure: true
      run:
        +tags: ["run", "advanced"]
        +materialized: incremental  # Best practice: large facts as incremental
        incremental_with_macros:
          +tags: ["incremental", "macros"]
          +materialized: incremental
        incremental_without_macros:
          +tags: ["incremental", "no_macros"]
          +materialized: incremental
    
    # Other Layer - Utility and reference models
    other:
      +tags: ["other", "utility", "reference"]
      +schema: other
      +materialized: table
      tpc_h_benchmarks:
        +tags: ["other", "benchmarks", "tpc_h"]
        +materialized: table
    

  # The dbt_artifacts configuration here is for logging your executions
  dbt_artifacts:
    +enabled: false
    +database: "{{ env_var('DBT_ARTIFACTS_DATABASE', target.database ) }}" # optional, default is your target database
    +schema: "{{ env_var('DBT_ARTIFACTS_SCHEMA', target.schema ) }}" # optional, default is your target schema
    sources:
      +enabled: true
      +materialized: incremental

snapshots:
  # The snowflake__snapshot_hash_arguments macro in this project removes MD5 from the dbt_scd_id
  # Below is an expensive one-time update macro that can fix any previously created snapshot records
  # This should be removed after it has executed at least once
  # pre-hook: "{{ refresh_dbt_scd_id() }}"
  +meta:
    owner: "Dan Flippo"
    owner_email: "test@nowhere.com"
  snowflake_demo:
    +schema: "DBT_DEMO_SNAPSHOTS"
    +tags: ["snapshots", "scd"]

tests:
  # You can set custom properties on tests that will be stored in the dbt logs
  +meta:
    owner: "Dan Flippo"
    owner_email: "test@nowhere.com"
#  +limit: 1000 # will only include the first 1000 failures
#  +store_failures: true # by default we will store failures
