---
description: dbt Observability using dbt Projects on Snowflake and dbt Artifacts package
alwaysApply: false
---

# dbt Observability Guide

This guide covers two approaches to dbt observability: **dbt Projects on Snowflake** (native Snowflake monitoring) and **dbt Artifacts Package** (cross-platform dbt logging).

---

## Section 1: dbt Projects on Snowflake Monitoring

**Reference:** [Snowflake dbt Projects Monitoring](https://docs.snowflake.com/en/user-guide/data-engineering/dbt-projects-on-snowflake-monitoring-observability)

### Overview

dbt Projects on Snowflake is a native feature that allows you to deploy and execute dbt projects as Snowflake objects. **Monitoring is performed through event tables that capture detailed telemetry data** including logs, traces, and metrics during execution.

**Reference**: [Event Table Setup](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up#associate-an-event-table-with-an-object)

### Key Monitoring Approach

**Event Tables** - Query event tables for logs, spans, and metrics generated during dbt project execution. Event tables capture comprehensive telemetry data including execution logs, performance metrics, and trace spans.

### Setup Event Table Monitoring

**IMPORTANT**: Event tables should be set at the **DATABASE** level (not schema, not account-wide):

```sql
-- 1. Create event table in the same or a different database as your dbt Project objects.
CREATE EVENT TABLE IF NOT EXISTS MY_LOGGING_DATABASE.MY_LOGGING_SCHEMA.EVENT_LOG;

-- 2. Set event table where dbt Projects are deployed at DATABASE level
ALTER DATABASE MY_DBT_PROJECT_DATABASE SET EVENT_TABLE = MY_LOGGING_DATABASE.MY_LOGGING_SCHEMA.EVENT_LOG;

-- 3. Configure logging levels for the schema where you have deployed your dbt Project object
ALTER SCHEMA MY_DBT_PROJECT_DATABASE.MY_DBT_PROJECT_SCHEMA SET LOG_LEVEL = 'INFO';
ALTER SCHEMA MY_DBT_PROJECT_DATABASE.MY_DBT_PROJECT_SCHEMA SET TRACE_LEVEL = 'ALWAYS';
ALTER SCHEMA MY_DBT_PROJECT_DATABASE.MY_DBT_PROJECT_SCHEMA SET METRIC_LEVEL = 'ALL';

```

### Deployment

```sql
-- Deploy dbt project using Snowflake CLI
snow dbt deploy PROJECT_NAME --connection default --database DB_NAME --schema SCHEMA_NAME

-- Execute dbt project full refresh
EXECUTE DBT PROJECT DB_NAME.SCHEMA_NAME.PROJECT_NAME args='build --full-refresh';

-- Execute with specific selection
EXECUTE DBT PROJECT DB_NAME.SCHEMA_NAME.PROJECT_NAME args='build --select tag:gold';
```

### Event Table Structure

Event tables use the [OpenTelemetry data model](https://opentelemetry.io/) with these key columns:

- **TIMESTAMP**: The UTC timestamp when an event was created. For events representing a span of time, this is the end of the time span.
- **START_TIMESTAMP**: For events representing a span of time, such as trace events, the start of the time span as a UTC timestamp
- **TRACE**: Tracing context for all signal types. Contains string values trace_id and span_id.
- **RESOURCE_ATTRIBUTES**: Attributes that identify the source of an event such as database, schema, user, warehouse, Openflow, etc.
- **SCOPE**: Scopes for events. For example, class names for logs.
- **RECORD_TYPE**: The event type. One of the following:
    - **LOG**: for a log message.
    - **SPAN**: for user-defined function invocations performed sequentially on the same thread. For more information, see RECORD_TYPE column.
    - **SPAN_EVENT**: for a single trace event. A single query can emit more than one SPAN_EVENT.
    - **EVENT**: for an event associated with an operation such as Iceberg automated refresh.
- **RECORD**: JSON object with record-specific data (severity, metric type, span details)
- **RECORD_ATTRIBUTES**: Describes the event with metadata set by Snowflake or by code. The value will vary depending on the type of record the row contains
- **VALUE**: Actual log message, metric value, or null for spans

### Monitoring Queries

#### Recent dbt Project Executions

```sql
-- Query event table for recent dbt project executions
SELECT 
    TIMESTAMP,
    RESOURCE_ATTRIBUTES['snow.executable.name']::VARCHAR AS project_name,
    RESOURCE_ATTRIBUTES['snow.database.name']::VARCHAR AS database_name,
    RESOURCE_ATTRIBUTES['snow.schema.name']::VARCHAR AS schema_name,
    RESOURCE_ATTRIBUTES['db.user']::VARCHAR AS user_name,
    RESOURCE_ATTRIBUTES['snow.owner.name']::VARCHAR AS owner_name,
    RESOURCE_ATTRIBUTES['snow.query.id']::VARCHAR AS query_id,
    RESOURCE_ATTRIBUTES['snow.session.role.primary.name']::VARCHAR AS primary_role_name,
    RESOURCE_ATTRIBUTES['snow.warehouse.name']::VARCHAR AS warehouse_name,
    RECORD['severity_text']::VARCHAR AS severity,
    VALUE::VARCHAR AS message
FROM MY_DATABASE.MY_SCHEMA.EVENT_LOG
WHERE RESOURCE_ATTRIBUTES['snow.executable.type']::VARCHAR = 'DBT_PROJECT'
  AND RECORD_TYPE = 'LOG'
  AND RECORD['severity_text']::VARCHAR IN ('ERROR', 'WARN', 'INFO')
  AND TIMESTAMP >= DATEADD(hour, -1, CURRENT_TIMESTAMP())
ORDER BY TIMESTAMP DESC;
```

#### dbt Project Execution Errors

```sql
-- Query for ERROR level logs
SELECT 
    TIMESTAMP,
    RESOURCE_ATTRIBUTES['snow.executable.name']::VARCHAR AS project_name,
    RESOURCE_ATTRIBUTES['snow.database.name']::VARCHAR AS database_name,
    RESOURCE_ATTRIBUTES['snow.schema.name']::VARCHAR AS schema_name,
    RESOURCE_ATTRIBUTES['snow.query.id']::VARCHAR AS query_id,
    RECORD['severity_text']::VARCHAR AS severity,
    VALUE::VARCHAR AS error_message
FROM MY_DATABASE.MY_SCHEMA.EVENT_LOG
WHERE RESOURCE_ATTRIBUTES['snow.executable.type']::VARCHAR = 'DBT_PROJECT'
  AND RECORD_TYPE = 'LOG'
  AND RECORD['severity_text']::VARCHAR IN ('ERROR')
  AND TIMESTAMP >= DATEADD(day, -7, CURRENT_TIMESTAMP())
ORDER BY TIMESTAMP DESC;
```

#### dbt Project Performance Metrics

```sql
-- Query performance metrics (CPU, memory usage)
SELECT 
    TIMESTAMP,
    RESOURCE_ATTRIBUTES['snow.executable.name']::VARCHAR AS project_name,
    RESOURCE_ATTRIBUTES['snow.database.name']::VARCHAR AS database_name,
    RESOURCE_ATTRIBUTES['snow.schema.name']::VARCHAR AS schema_name,
    RESOURCE_ATTRIBUTES['snow.query.id']::VARCHAR AS query_id,
    RECORD['metric']['name']::VARCHAR AS metric_name,
    RECORD['metric']['unit']::VARCHAR AS metric_unit,
    RECORD['metric_type']::VARCHAR AS metric_type,
    RECORD['value_type']::VARCHAR AS value_type,
    VALUE::FLOAT AS metric_value,
    RESOURCE_ATTRIBUTES
FROM MY_DATABASE.MY_SCHEMA.EVENT_LOG
WHERE RESOURCE_ATTRIBUTES['snow.executable.type']::VARCHAR = 'DBT_PROJECT'
  AND RECORD_TYPE = 'METRIC'
  AND TIMESTAMP >= DATEADD(hour, -1, CURRENT_TIMESTAMP())
ORDER BY TIMESTAMP DESC;
```

#### dbt Project Trace Spans

```sql
-- Query execution spans for tracing
SELECT 
    RESOURCE_ATTRIBUTES['snow.executable.name']::VARCHAR AS project_name,
    RESOURCE_ATTRIBUTES['snow.database.name']::VARCHAR AS database_name,
    RESOURCE_ATTRIBUTES['snow.schema.name']::VARCHAR AS schema_name,
    start_timestamp,
    TIMESTAMP,
    timestampdiff(milliseconds, start_timestamp, timestamp)::integer as milliseconds_duration,
    round(milliseconds_duration / 1000, 3) as seconds_duration,
    round(milliseconds_duration / 60000, 2) as minutes_duration,
    RECORD['name']::VARCHAR AS span_name,
    TRACE['trace_id']::VARCHAR AS trace_id,
    TRACE['span_id']::VARCHAR AS span_id,
    RESOURCE_ATTRIBUTES,
    RECORD,
    RECORD_ATTRIBUTES,
    SCOPE
FROM MY_DATABASE.MY_SCHEMA.EVENT_LOG l
WHERE l.RESOURCE_ATTRIBUTES['snow.executable.type']::VARCHAR = 'DBT_PROJECT'
  AND l.RECORD_TYPE = 'SPAN'
  AND l.TIMESTAMP >= DATEADD(hour, -12, CURRENT_TIMESTAMP())
ORDER BY l.TIMESTAMP DESC;
```

#### dbt Execution Summary by Project

```sql
-- Summarize executions by project
SELECT 
    RESOURCE_ATTRIBUTES['snow.executable.name']::VARCHAR AS project_name,
    RESOURCE_ATTRIBUTES['snow.database.name']::VARCHAR AS database_name,
    RESOURCE_ATTRIBUTES['snow.schema.name']::VARCHAR AS schema_name,
    COUNT(DISTINCT RESOURCE_ATTRIBUTES['snow.query.id']::VARCHAR) AS execution_count,
    MIN(TIMESTAMP) AS first_execution,
    MAX(TIMESTAMP) AS last_execution,
    COUNT(CASE WHEN RECORD['severity_text']::VARCHAR = 'ERROR' THEN 1 END) AS error_count,
    COUNT(CASE WHEN RECORD['severity_text']::VARCHAR = 'WARN' THEN 1 END) AS warning_count
FROM MY_DATABASE.MY_SCHEMA.EVENT_LOG
WHERE RESOURCE_ATTRIBUTES['snow.executable.type']::VARCHAR = 'DBT_PROJECT'
  AND RECORD_TYPE = 'LOG'
  AND TIMESTAMP >= DATEADD(day, -7, CURRENT_TIMESTAMP())
GROUP BY project_name, database_name, schema_name
ORDER BY last_execution DESC;
```

---

## Section 2: dbt Artifacts Package

**Reference:** [brooklyn-data/dbt_artifacts](https://github.com/brooklyn-data/dbt_artifacts)

### Overview

The dbt_artifacts package captures metadata during dbt execution and stores it in 11 source tables. These tables provide detailed execution history for models, tests, seeds, and snapshots.

### Installation

```yaml
# packages.yml
packages:
  - package: brooklyn-data/dbt_artifacts
    version: 2.9.3

# dbt_project.yml
on-run-end:
  - "{{ dbt_artifacts.upload_results(results) }}"

models:
  dbt_artifacts:
    +database: your_database
    +schema: dbt_artifacts
```

```bash
dbt deps
dbt run --select dbt_artifacts
```

### Source Tables Reference

#### Table: `invocations`
**Description:** One row per dbt run. Links all executions together.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Unique ID for this dbt run |
| dbt_version | VARCHAR | dbt version used |
| project_name | VARCHAR | Project name |
| run_started_at | TIMESTAMP | When the run started |
| dbt_command | VARCHAR | Command executed (run, test, build) |
| full_refresh_flag | BOOLEAN | Was --full-refresh used? |
| target_profile_name | VARCHAR | Profile name from profiles.yml |
| target_name | VARCHAR | Target environment (dev, prod) |
| target_schema | VARCHAR | Schema used for this run |
| target_threads | NUMBER | Thread count |
| dbt_cloud_project_id | VARCHAR | dbt Cloud project ID (if applicable) |
| dbt_cloud_job_id | VARCHAR | dbt Cloud job ID |
| dbt_cloud_run_id | VARCHAR | dbt Cloud run ID |
| env_vars | VARIANT | Environment variables |
| dbt_vars | VARIANT | dbt variables passed |
| invocation_args | VARIANT | Command-line arguments |

#### Table: `models`
**Description:** One row per model per run. Captures model metadata and configuration.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Unique model identifier |
| run_started_at | TIMESTAMP | Run timestamp |
| database | VARCHAR | Target database |
| schema | VARCHAR | Target schema |
| name | VARCHAR | Model name |
| depends_on_nodes | ARRAY | Upstream dependencies |
| package_name | VARCHAR | Package name |
| path | VARCHAR | File path to model |
| checksum | VARCHAR | File checksum (detects changes) |
| materialization | VARCHAR | table, view, incremental, etc. |
| tags | ARRAY | Model tags |
| meta | VARIANT | Model metadata |
| alias | VARCHAR | Model alias |
| all_results | VARIANT | All execution results |

#### Table: `model_executions`
**Description:** One row per model execution. Captures runtime and results.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Links to models |
| run_started_at | TIMESTAMP | Run timestamp |
| was_full_refresh | BOOLEAN | Full refresh execution? |
| thread_id | VARCHAR | Thread that executed the model |
| status | VARCHAR | success, error, skipped |
| compile_started_at | TIMESTAMP | Compilation start time |
| query_completed_at | TIMESTAMP | Query completion time |
| total_node_runtime | FLOAT | Execution time (seconds) |
| rows_affected | NUMBER | Rows inserted/updated |
| materialization | VARCHAR | Materialization type |
| schema | VARCHAR | Target schema |
| name | VARCHAR | Model name |
| alias | VARCHAR | Model alias |
| message | VARCHAR | Error or status message |
| adapter_response | VARIANT | Warehouse-specific response |

#### Table: `tests`
**Description:** One row per test per run. Captures test metadata.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Unique test identifier |
| run_started_at | TIMESTAMP | Run timestamp |
| name | VARCHAR | Test name |
| depends_on_nodes | ARRAY | Nodes being tested |
| package_name | VARCHAR | Package name |
| test_path | VARCHAR | File path to test |
| tags | ARRAY | Test tags |
| all_results | VARIANT | All execution results |

#### Table: `test_executions`
**Description:** One row per test execution. PRIMARY TABLE FOR DATA QUALITY.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Links to tests |
| run_started_at | TIMESTAMP | Run timestamp |
| was_full_refresh | BOOLEAN | Full refresh execution? |
| thread_id | VARCHAR | Thread that executed the test |
| status | VARCHAR | pass, fail, warn, error, skipped |
| compile_started_at | TIMESTAMP | Compilation start time |
| query_completed_at | TIMESTAMP | Query completion time |
| total_node_runtime | FLOAT | Execution time (seconds) |
| rows_affected | NUMBER | Rows returned by test query |
| failures | NUMBER | Number of failing rows (0 = pass) |
| message | VARCHAR | Error or failure message |
| adapter_response | VARIANT | Warehouse-specific response |

#### Table: `seeds`
**Description:** One row per seed per run. Captures seed metadata.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Unique seed identifier |
| run_started_at | TIMESTAMP | Run timestamp |
| database | VARCHAR | Target database |
| schema | VARCHAR | Target schema |
| name | VARCHAR | Seed name |
| package_name | VARCHAR | Package name |
| path | VARCHAR | File path to seed CSV |
| checksum | VARCHAR | File checksum |
| meta | VARIANT | Seed metadata |
| alias | VARCHAR | Seed alias |
| all_results | VARIANT | All execution results |

#### Table: `seed_executions`
**Description:** One row per seed execution.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Links to seeds |
| run_started_at | TIMESTAMP | Run timestamp |
| was_full_refresh | BOOLEAN | Full refresh execution? |
| thread_id | VARCHAR | Thread that executed |
| status | VARCHAR | success, error, skipped |
| compile_started_at | TIMESTAMP | Compilation start time |
| query_completed_at | TIMESTAMP | Query completion time |
| total_node_runtime | FLOAT | Execution time (seconds) |
| rows_affected | NUMBER | Rows loaded |
| materialization | VARCHAR | Materialization type |
| schema | VARCHAR | Target schema |
| name | VARCHAR | Seed name |
| alias | VARCHAR | Seed alias |
| message | VARCHAR | Error or status message |
| adapter_response | VARIANT | Warehouse-specific response |

#### Table: `snapshots`
**Description:** One row per snapshot per run. Captures snapshot metadata.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Unique snapshot identifier |
| run_started_at | TIMESTAMP | Run timestamp |
| database | VARCHAR | Target database |
| schema | VARCHAR | Target schema |
| name | VARCHAR | Snapshot name |
| depends_on_nodes | ARRAY | Upstream dependencies |
| package_name | VARCHAR | Package name |
| path | VARCHAR | File path to snapshot |
| checksum | VARCHAR | File checksum |
| strategy | VARCHAR | Snapshot strategy (timestamp, check) |
| meta | VARIANT | Snapshot metadata |
| alias | VARCHAR | Snapshot alias |
| all_results | VARIANT | All execution results |

#### Table: `snapshot_executions`
**Description:** One row per snapshot execution.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Links to snapshots |
| run_started_at | TIMESTAMP | Run timestamp |
| was_full_refresh | BOOLEAN | Full refresh execution? |
| thread_id | VARCHAR | Thread that executed |
| status | VARCHAR | success, error, skipped |
| compile_started_at | TIMESTAMP | Compilation start time |
| query_completed_at | TIMESTAMP | Query completion time |
| total_node_runtime | FLOAT | Execution time (seconds) |
| rows_affected | NUMBER | Rows captured/updated |
| materialization | VARCHAR | Materialization type |
| schema | VARCHAR | Target schema |
| name | VARCHAR | Snapshot name |
| alias | VARCHAR | Snapshot alias |
| message | VARCHAR | Error or status message |
| adapter_response | VARIANT | Warehouse-specific response |

#### Table: `sources`
**Description:** One row per source per run. Captures source definitions.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Unique source identifier |
| run_started_at | TIMESTAMP | Run timestamp |
| database | VARCHAR | Source database |
| schema | VARCHAR | Source schema |
| name | VARCHAR | Source name |
| source_name | VARCHAR | Source system name |
| loader | VARCHAR | Data loader used |
| identifier | VARCHAR | Table identifier |
| all_results | VARIANT | All execution results |

#### Table: `exposures`
**Description:** One row per exposure per run. Captures exposure definitions.

| Column | Type | Description |
|--------|------|-------------|
| command_invocation_id | VARCHAR | Links to invocations |
| node_id | VARCHAR | Unique exposure identifier |
| run_started_at | TIMESTAMP | Run timestamp |
| name | VARCHAR | Exposure name |
| type | VARCHAR | Exposure type (dashboard, notebook, etc.) |
| owner | VARCHAR | Exposure owner |
| maturity | VARCHAR | Maturity level |
| path | VARCHAR | File path to exposure |
| description | VARCHAR | Exposure description |
| url | VARCHAR | Exposure URL |
| depends_on_nodes | ARRAY | Upstream dependencies |
| all_results | VARIANT | All execution results |

---

## Observability Queries

### dbt Artifacts: Test Quality Monitoring

```sql
-- Recent test failures
SELECT 
    i.run_started_at,
    i.dbt_command,
    i.target_name,
    t.name AS test_name,
    t.test_path,
    t.depends_on_nodes,
    te.status,
    te.failures,
    te.total_node_runtime AS execution_seconds,
    te.message
FROM test_executions te
JOIN invocations i 
    ON te.command_invocation_id = i.command_invocation_id
JOIN tests t 
    ON te.node_id = t.node_id 
    AND te.command_invocation_id = t.command_invocation_id
WHERE i.run_started_at >= DATEADD(day, -7, CURRENT_TIMESTAMP())
  AND te.status IN ('fail', 'error')
ORDER BY i.run_started_at DESC;
```

```sql
-- Test reliability metrics
SELECT 
    t.name AS test_name,
    COUNT(*) AS total_executions,
    SUM(CASE WHEN te.status = 'pass' THEN 1 ELSE 0 END) AS passes,
    SUM(CASE WHEN te.status = 'fail' THEN 1 ELSE 0 END) AS failures,
    ROUND(100.0 * SUM(CASE WHEN te.status = 'fail' THEN 1 ELSE 0 END) / COUNT(*), 2) AS failure_rate_pct,
    AVG(te.total_node_runtime) AS avg_runtime_sec
FROM test_executions te
JOIN tests t ON te.node_id = t.node_id
WHERE te.run_started_at >= DATEADD(day, -30, CURRENT_TIMESTAMP())
GROUP BY t.name
HAVING COUNT(*) >= 5
ORDER BY failure_rate_pct DESC, failures DESC;
```

### dbt Artifacts: Model Performance

```sql
-- Slowest models
SELECT 
    m.name AS model_name,
    m.materialization,
    COUNT(DISTINCT i.command_invocation_id) AS run_count,
    AVG(me.total_node_runtime) AS avg_runtime_sec,
    MAX(me.total_node_runtime) AS max_runtime_sec,
    AVG(me.rows_affected) AS avg_rows_affected,
    SUM(CASE WHEN me.status = 'error' THEN 1 ELSE 0 END) AS error_count
FROM model_executions me
JOIN models m 
    ON me.node_id = m.node_id 
    AND me.command_invocation_id = m.command_invocation_id
JOIN invocations i 
    ON me.command_invocation_id = i.command_invocation_id
WHERE i.run_started_at >= DATEADD(day, -30, CURRENT_TIMESTAMP())
GROUP BY m.name, m.materialization
HAVING COUNT(*) >= 5
ORDER BY avg_runtime_sec DESC
LIMIT 20;
```

```sql
-- Model execution errors
SELECT 
    i.run_started_at,
    i.target_name,
    m.name AS model_name,
    m.materialization,
    me.status,
    me.total_node_runtime AS execution_seconds,
    me.rows_affected,
    me.message
FROM model_executions me
JOIN invocations i 
    ON me.command_invocation_id = i.command_invocation_id
JOIN models m 
    ON me.node_id = m.node_id 
    AND me.command_invocation_id = m.command_invocation_id
WHERE i.run_started_at >= DATEADD(day, -7, CURRENT_TIMESTAMP())
  AND me.status != 'success'
ORDER BY i.run_started_at DESC;
```

### dbt Artifacts: Run History

```sql
-- dbt run summary
SELECT 
    run_started_at,
    dbt_version,
    dbt_command,
    target_name,
    target_schema,
    target_warehouse,
    full_refresh_flag
FROM invocations
WHERE run_started_at >= DATEADD(day, -30, CURRENT_TIMESTAMP())
ORDER BY run_started_at DESC;
```

---

## Best Practices

### Choosing an Approach

**Use dbt Projects on Snowflake when:**
- You want native Snowflake integration
- You're deploying dbt entirely within Snowflake
- You prefer serverless execution
- You want to leverage Snowflake's built-in near-real-time monitoring

**Use dbt Artifacts Package when:**
- You need cross-platform compatibility
- You want detailed test and model execution history
- You need programmatic access to execution metadata
- You're running dbt from external orchestration tools
- You can wait until the entire run succeeds or fails to get status

**Use Both when:**
- You want comprehensive monitoring
- You need both native Snowflake monitoring and detailed execution logs
- You have complex data quality requirements

### Performance Optimization

- Query event tables with time filters (they can be large)
- Use `command_invocation_id` to link dbt_artifacts tables
- Archive old execution data (>90 days) to separate tables

---

## References

- [dbt Projects on Snowflake Documentation](https://docs.snowflake.com/en/user-guide/data-engineering/dbt-projects-on-snowflake)
- [dbt Projects Monitoring](https://docs.snowflake.com/en/user-guide/data-engineering/dbt-projects-on-snowflake-monitoring-observability)
- [dbt_artifacts GitHub](https://github.com/brooklyn-data/dbt_artifacts)
- [dbt_artifacts Documentation](https://brooklyn-data.github.io/dbt_artifacts)
- [Snowflake Query History](https://docs.snowflake.com/en/sql-reference/account-usage/query_history)
