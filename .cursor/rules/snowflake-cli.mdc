---
description: Comprehensive guide for using Snowflake CLI (snow) commands in development workflows
globs: **/*.py, **/*.sql, **/*.yml, **/*.yaml, **/snowflake.yml
alwaysApply: true
---

# Snowflake CLI (snow) Usage Guide

## **Connection Configuration**

- **Default Connection**: Use `default` as the connection name unless specified otherwise
- **Connection Override**: Allow users to specify a different connection name at the top of their project
- **Connection Parameter**: Always use `--connection <connection_name>` or `-c <connection_name>` when executing snow commands

```bash
# Default usage
snow sql -q "SELECT CURRENT_USER()" --connection default

# Custom connection
snow sql -q "SELECT CURRENT_USER()" --connection my_dev_connection
```

## **Global Options Available Across All Commands**

- **`--help, -h`**: Show help message and exit
- **`--version`**: Show Snowflake CLI version
- **`--info`**: Show information about Snowflake CLI
- **`--config-file FILE`**: Specify custom configuration file
- **`--format [TABLE|JSON|JSON_EXT|CSV]`**: Specify output format (default: TABLE)
- **`--verbose, -v`**: Display log entries for info level and higher
- **`--debug`**: Display debug level log entries with additional information
- **`--silent`**: Turn off intermediate output to console
- **`--enhanced-exit-codes`**: Differentiate exit codes based on failure type

## **Connection Configuration Options**

Most commands support these connection override options:
- **`--connection, --environment, -c TEXT`**: Connection name from config.toml (default: default)
- **`--host TEXT`**: Host address override
- **`--port INTEGER`**: Port override
- **`--account, --accountname TEXT`**: Snowflake account name override
- **`--user, --username TEXT`**: Username override
- **`--password TEXT`**: Password override
- **`--authenticator TEXT`**: Authenticator override
- **`--database, --dbname TEXT`**: Database override
- **`--schema, --schemaname TEXT`**: Schema override
- **`--role, --rolename TEXT`**: Role override
- **`--warehouse TEXT`**: Warehouse override
- **`--temporary-connection, -x`**: Use command line parameters instead of config

## **Command Categories and Usage**

### **SQL Execution (`snow sql`)**

**Purpose**: Execute SQL queries against Snowflake
**Common Use Cases**: Running queries, executing scripts, database operations

```bash
# Execute inline query
snow sql -q "SELECT CURRENT_DATABASE()" -c default

# Execute SQL file
snow sql -f path/to/script.sql -c default

# Execute from stdin
cat query.sql | snow sql -i -c default

# Execute with variables
snow sql -q "SELECT * FROM {{table_name}}" -D table_name=my_table -c default

# Execute multi-line SQL using heredoc (EOF)
snow sql -c default -i <<EOF
CREATE OR REPLACE FILE FORMAT MY_CSV_FORMAT
    TYPE = CSV
    SKIP_HEADER = 1;

CREATE OR REPLACE TABLE DIAMONDS (
    "ROW" NUMBER(38,0),
    CARAT FLOAT,
    CUT VARCHAR,
    COLOR VARCHAR,
    CLARITY VARCHAR,
    DEPTH FLOAT,
    "TABLE" FLOAT,
    PRICE NUMBER(38,0),
    X FLOAT,
    Y FLOAT,
    Z FLOAT
);

-- Upload local csv to table stage
PUT file://diamonds.csv @%DIAMONDS OVERWRITE=TRUE;

-- Copy data using the file format
COPY INTO DIAMONDS FILE_FORMAT = ( FORMAT_NAME = 'MY_CSV_FORMAT' );
EOF
```

**Key Options**:
- **`--query, -q TEXT`**: SQL query to execute
- **`--filename, -f FILE`**: SQL file to execute
- **`--stdin, -i`**: Read query from standard input
- **`--variable, -D TEXT`**: Variables in key=value format for templating
- **`--retain-comments`**: Keep comments in queries
- **`--single-transaction/--no-single-transaction`**: Execute as single transaction
- **`--enable-templating [LEGACY|STANDARD|JINJA|ALL|NONE]`**: Template syntax
- **`--project, -p TEXT`**: Project path
- **`--env TEXT`**: Environment variables for templates

### **Stage Management (`snow stage`)**

**Purpose**: Manage Snowflake stages for file operations
**Common Use Cases**: Executing staged files, file uploads/downloads, stage operations

```bash
# Create stage
snow stage create my_stage -c default

# Upload files to stage
snow stage copy ./local_files/ @my_stage/ -c default

# Download from stage
snow stage copy @my_stage/file.csv ./downloads/ -c default

# List stage contents
snow stage list-files @my_stage -c default

# Execute SQL files from stage
snow stage execute @my_stage/scripts/*.sql -c default -D "database='DFLIPPO_DEV'" -D "schema='TESTING'" -D "postgresql_host='host.docker.internal'"

```
#### **Special Notes for `snow stage execute`**:
- For .sql files, `snow stage execute` performs an EXECUTE IMMEDIATE FROM command on .sql files from a stage.
- For .py files, `snow stage execute` runs as a session-scoped Snowpark Python procedure. 
    - Snowflake CLI executes the procedure in Snowflake to guarantee a consistent execution environment. 
    - If your Python scripts require additional requirements, you should specify them in a requirements.txt file that resides in the same directory as the files on the stage. 
    - The snow stage execute command only supports packages from the Snowflake Anaconda channel. 
    - By default, the command looks for the requirements.txt file in the following precedence:
        1) Stage path specified in the command’s stage_path parameter.
        2) Parent directories of the specified stage path hierarchy, until it reaches the stage.
        3) If you don’t specify a requirements.txt file, the command assumes no additional packages are necessary.
- Variables for the execution context; for example: -D "<key>=<value>".
    - Variable names are case sensitive and must match the case in your jinja expressions
    - For SQL files, variables are used to expand the template, and any unknown variable will cause an error (consider embedding quoting in the file).
    - For Python files, variables are used to update the os.environ dictionary. 
    - Provided keys are capitalized to adhere to best practices. In case of SQL files string values must be quoted in '' (consider embedding quoting in the file).

**Subcommands**:
- **`copy`**: Upload/download files to/from stages
- **`create`**: Create named stages
- **`describe`**: Get stage description
- **`drop`**: Drop stages
- **`execute`**: Execute SQL files from stage (supports glob patterns)
- **`list`**: List all stages
- **`list-files`**: List stage contents
- **`remove`**: Remove files from stage

### **Snowpark Development (`snow snowpark`)**

**Purpose**: Manage Snowpark procedures and functions
**Common Use Cases**: Deploying UDFs, stored procedures, managing dependencies

```bash
# Build Snowpark artifacts
snow snowpark build -c default

# Deploy procedures and functions
snow snowpark deploy --replace -c default

# Execute procedure
snow snowpark execute procedure my_proc(arg1, arg2) -c default

# List procedures
snow snowpark list procedure -c default

# Describe function
snow snowpark describe function my_func -c default
```

**Subcommands**:
- **`build`**: Build artifacts for Snowpark project
- **`deploy`**: Deploy procedures/functions (use `--replace` to overwrite)
- **`describe`**: Get procedure/function description
- **`drop`**: Drop procedure/function
- **`execute`**: Execute procedure/function
- **`list`**: List procedures/functions
- **`package`**: Manage custom Python packages

### **Object Management (`snow object`)**

**Purpose**: Manage Snowflake objects like warehouses, databases, schemas, tables
**Common Use Cases**: Creating/dropping objects, listing resources, describing objects

```bash
# List warehouses
snow object list warehouse -c default

# Create warehouse
snow object create warehouse --name my_warehouse --size SMALL -c default

# Describe table
snow object describe table my_table -c default

# Drop object
snow object drop table my_table -c default
```

**Subcommands**:
- **`create`**: Create objects (check docs for supported types and parameters)
- **`describe`**: Get object description
- **`drop`**: Drop objects
- **`list`**: List available objects of given type

### **Streamlit Applications (`snow streamlit`)**

**Purpose**: Manage Streamlit apps in Snowflake
**Common Use Cases**: Deploying apps, managing app lifecycle

```bash
# Deploy Streamlit app
snow streamlit deploy -c default

# Get app URL
snow streamlit get-url my_app -c default

# List Streamlit apps
snow streamlit list -c default

# Execute app in headless mode
snow streamlit execute my_app -c default
```

**Subcommands**:
- **`deploy`**: Deploy Streamlit app from project definition
- **`describe`**: Get app description
- **`drop`**: Drop Streamlit app
- **`execute`**: Execute app in headless mode
- **`get-url`**: Get app URL
- **`list`**: List all Streamlit apps
- **`share`**: Share app with another role

### **Native App Framework (`snow app`)**

**Purpose**: Manage Snowflake Native Apps
**Common Use Cases**: App development lifecycle, publishing, versioning

```bash
# Bundle app artifacts
snow app bundle -c default

# Deploy app package
snow app deploy --prune --recursive -c default

# Run app (deploy + create/upgrade)
snow app run -c default

# Open app in browser
snow app open -c default

# Teardown app
snow app teardown -c default
```

**Subcommands**:
- **`bundle`**: Prepare local folder with app artifacts
- **`deploy`**: Create app package and sync changes
- **`events`**: Fetch app events from event table
- **`open`**: Open app in browser
- **`publish`**: Add version to release channel
- **`release-channel`**: Manage release channels
- **`release-directive`**: Manage release directives
- **`run`**: Full deployment (package + app creation/upgrade)
- **`teardown`**: Drop app object and package
- **`validate`**: Validate app setup script
- **`version`**: Manage app versions

### **Container Services (`snow spcs`)**

**Purpose**: Manage Snowpark Container Services
**Common Use Cases**: Container deployment, compute pool management, image management

```bash
# Manage compute pools
snow spcs compute-pool list -c default

# Manage services
snow spcs service list -c default

# Manage image registries
snow spcs image-registry list -c default

# Manage image repositories
snow spcs image-repository list -c default
```

**Subcommands**:
- **`compute-pool`**: Manage compute pools
- **`image-registry`**: Manage image registries
- **`image-repository`**: Manage image repositories
- **`service`**: Manage services

### **Notebook Management (`snow notebook`)**

**Purpose**: Manage Snowflake Notebooks
**Common Use Cases**: Notebook deployment, execution, URL generation

```bash
# Deploy notebook
snow notebook deploy my_notebook -c default

# Execute notebook in headless mode
snow notebook execute my_notebook -c default

# Get notebook URL
snow notebook get-url my_notebook -c default

# Open notebook in browser
snow notebook open my_notebook -c default
```

**Subcommands**:
- **`create`**: Create notebook from stage
- **`deploy`**: Upload and create Snowflake notebook
- **`execute`**: Execute notebook in headless mode
- **`get-url`**: Get notebook URL
- **`open`**: Open notebook in browser

### **Git Integration (`snow git`)**

**Purpose**: Manage Git repositories in Snowflake
**Common Use Cases**: Repository setup, file operations, script execution

```bash
# Setup git repository
snow git setup my_repo --url https://github.com/user/repo.git -c default

# List repositories
snow git list -c default

# Fetch changes
snow git fetch my_repo -c default

# List files in repository
snow git list-files my_repo --path branches/main -c default

# Execute SQL files from repository
snow git execute @my_repo/branches/main/*.sql -c default

# Copy files from repository
snow git copy @my_repo/branches/main/scripts ./local_scripts -c default
```

**Subcommands**:
- **`copy`**: Copy files from repository to local/stage
- **`describe`**: Get repository description
- **`drop`**: Drop git repository
- **`execute`**: Execute files from repository (supports glob patterns)
- **`fetch`**: Fetch changes from origin
- **`list`**: List all repositories
- **`list-branches`**: List repository branches
- **`list-files`**: List files in repository
- **`list-tags`**: List repository tags
- **`setup`**: Setup git repository object

### **Connection Management (`snow connection`)**

**Purpose**: Manage Snowflake connections
**Common Use Cases**: Connection setup, testing, management

```bash
# List connections
snow connection list

# Add new connection
snow connection add my_connection

# Test connection
snow connection test my_connection

# Set default connection
snow connection set-default my_connection

# Remove connection
snow connection remove my_connection

# Generate JWT token
snow connection generate-jwt my_connection
```

**Subcommands**:
- **`add`**: Add connection to configuration
- **`generate-jwt`**: Generate JWT token
- **`list`**: List configured connections
- **`remove`**: Remove connection from configuration
- **`set-default`**: Change default connection
- **`test`**: Test connection to Snowflake

### **Authentication (`snow auth`)**

**Purpose**: Manage authentication methods
**Common Use Cases**: OIDC authentication setup

```bash
# Manage OIDC authentication
snow auth oidc --help
```

**Subcommands**:
- **`oidc`**: Manage OIDC authentication

### **Cortex AI (`snow cortex`)**

**Purpose**: Access Snowflake Cortex AI functions
**Common Use Cases**: AI-powered text processing, analysis

```bash
# Complete text using AI
snow cortex complete "Explain machine learning" -c default

# Summarize text
snow cortex summarize "Long text to summarize..." -c default

# Analyze sentiment
snow cortex sentiment "I love this product!" -c default

# Translate text
snow cortex translate "Hello" --target-language spanish -c default

# Extract answers from documents
snow cortex extract-answer "What is the main topic?" --document "Document text..." -c default
```

**Subcommands**:
- **`complete`**: Generate text completions using language models
- **`extract-answer`**: Extract answers from documents
- **`sentiment`**: Analyze sentiment (-1 to 1 scale)
- **`summarize`**: Summarize English text
- **`translate`**: Translate text between languages

### **Logging (`snow logs`)**

**Purpose**: Retrieve logs for Snowflake objects
**Common Use Cases**: Debugging, monitoring, troubleshooting

```bash
# Get logs for compute pool
snow logs compute-pool my_pool -c default

# Get logs with time range
snow logs service my_service --from 2024-01-01T00:00:00 --to 2024-01-02T00:00:00 -c default

# Stream logs with refresh
snow logs compute-pool my_pool --refresh 5 -c default

# Filter by log level
snow logs service my_service --log-level ERROR -c default
```

**Key Options**:
- **`--from TEXT`**: Start time (ISO8061 format)
- **`--to TEXT`**: End time (ISO8061 format)
- **`--refresh INTEGER`**: Stream logs with refresh interval in seconds
- **`--table TEXT`**: Custom table to query for logs
- **`--log-level TEXT`**: Filter by log level (default: INFO)

### **Project Initialization (`snow init`)**

**Purpose**: Create new projects from templates
**Common Use Cases**: Starting new Snowflake projects

```bash
# Initialize project from default templates
snow init my_project

# Initialize with specific template
snow init my_project --template streamlit

# Initialize with custom template source
snow init my_project --template-source https://github.com/my-org/templates

# Initialize with variables
snow init my_project -D project_name=MyApp -D author=John

# Non-interactive initialization
snow init my_project --no-interactive
```

**Key Options**:
- **`--template TEXT`**: Specific template to use
- **`--template-source TEXT`**: Template source (local path or git URL)
- **`--variable, -D TEXT`**: Variables in key=value format
- **`--no-interactive`**: Disable prompting

### **Helper Commands (`snow helpers`)**

**Purpose**: Utility commands for migration and setup
**Common Use Cases**: Migration from SnowSQL, environment checks

```bash
# Check SnowSQL environment variables
snow helpers check-snowsql-env-vars

# Import SnowSQL connections
snow helpers import-snowsql-connections

# Migrate V1 to V2 project definitions
snow helpers v1-to-v2
```

**Subcommands**:
- **`check-snowsql-env-vars`**: Check for SnowSQL environment variables
- **`import-snowsql-connections`**: Import existing SnowSQL connections
- **`v1-to-v2`**: Migrate project definitions from V1 to V2

## **Best Practices for Cursor Integration**

### **Command Construction**
- Always include connection parameter: `--connection <connection_name>`
- Use appropriate output format for parsing: `--format JSON` for programmatic use
- Include error handling with `--enhanced-exit-codes`
- Use `--silent` for background operations to reduce noise
- Use heredoc (EOF) syntax for multi-line SQL scripts in shell scripts

### **Error Handling**
- Check exit codes for command success/failure
- Use `--debug` flag when troubleshooting
- Capture both stdout and stderr for comprehensive error reporting

### **Security Considerations**
- Never hardcode passwords or sensitive information in commands
- Use connection profiles instead of inline credentials
- Leverage temporary connections (`-x`) for one-off operations when appropriate

### **Performance Optimization**
- Use `--single-transaction` for related SQL operations
- Batch operations when possible
- Use appropriate warehouse sizes for different operations

### **Project Structure Integration**
- Respect `snowflake.yml` project definitions
- Use relative paths for local files
- Maintain consistent naming conventions for stages and objects

## **Common Patterns**

### **Development Workflow**
```bash
# 1. Test connection
snow connection test default

# 2. Execute development SQL
snow sql -f dev/setup.sql -c default

# 3. Deploy Snowpark functions
snow snowpark deploy --replace -c default

# 4. Run tests
snow sql -f tests/test_functions.sql -c default
```

### **Data Pipeline Operations**
```bash
# 1. Upload data files
snow stage copy ./data/ @my_stage/raw/ -c default

# 2. Execute transformation SQL
snow sql -f transforms/process_data.sql -c default

# 3. Verify results
snow sql -q "SELECT COUNT(*) FROM processed_data" -c default
```

### **Multi-line SQL Deployment with Heredoc**
```bash
# Deploy schema and load data using heredoc
snow sql --connection "${SNOWFLAKE_CONNECTION}" \
    --role "${SNOWFLAKE_ROLE}" \
    --database "${DATABASE_NAME}" \
    --schema "${SCHEMA_NAME}" \
    --warehouse "${WAREHOUSE_NAME}" -i <<EOF

-- Create file format
CREATE OR REPLACE FILE FORMAT ${FILE_FORMAT_NAME}
TYPE = CSV
FIELD_DELIMITER = ','
SKIP_HEADER = 1
NULL_IF = ('NULL', 'null', '')
EMPTY_FIELD_AS_NULL = TRUE;

-- Create target table
CREATE OR REPLACE TABLE ${TABLE_NAME} (
    id NUMBER AUTOINCREMENT,
    name VARCHAR(100),
    created_date TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
    status VARCHAR(20) DEFAULT 'ACTIVE'
);

-- Upload and load data
PUT file://${LOCAL_FILE_PATH} @%${TABLE_NAME} OVERWRITE=TRUE;
COPY INTO ${TABLE_NAME} FROM @%${TABLE_NAME} 
FILE_FORMAT = (FORMAT_NAME = '${FILE_FORMAT_NAME}');

-- Verify load
SELECT COUNT(*) as record_count FROM ${TABLE_NAME};

EOF
```

### **CI/CD Integration**
```bash
# 1. Validate connection
snow connection test $SNOWFLAKE_CONNECTION

# 2. Deploy changes
snow snowpark deploy --replace -c $SNOWFLAKE_CONNECTION

# 3. Run integration tests
snow sql -f tests/integration.sql -c $SNOWFLAKE_CONNECTION --format JSON
```

## **Troubleshooting**

### **Common Issues**
- **Connection failures**: Check `snow connection test <name>`
- **Permission errors**: Verify role and warehouse settings
- **File not found**: Ensure correct paths and stage references
- **SQL errors**: Use `--debug` flag for detailed error information

### **Debug Commands**
```bash
# Test connection with diagnostics
snow connection test default --enable-diag

# Execute with debug logging
snow sql -q "SELECT 1" -c default --debug

# Check CLI information
snow --info
```

---

**Reference**: [Snowflake CLI Documentation](https://docs.snowflake.com/en/developer-guide/snowflake-cli/index)